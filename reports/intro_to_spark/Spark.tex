\documentclass[9pt,twocolumn,twoside]{idsi}
% Defines a new command for the horizontal lines, change thickness here
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 

\renewcommand{\headrulewidth}{2pt}
\fancypagestyle{plain}{%
  \fancyhead[L]{
    \begin{tabular}{ll}
%        \includegraphics[scale=0.15]{figs/ncsa_vertical} 
    \end{tabular}
  }
  \fancyhead[C]{
      	\begin{tabular}[m]{c}
		  	\fontsize{20}{20} Illinois Data Science Initiative    	
		\end{tabular}
  }

  \fancyhead[R]{
    \begin{tabular}{ll}
%	  	\includegraphics[scale=0.125]{figs/ill}  		
  	\end{tabular}
  }
  
  \fancyfoot[C]{\thepage}
}
\pagestyle{plain}
\def \report_title {Setting up Spark for Python}
\author[1]{Sameet Sapra}
\author[2]{Joshua Chang}
\author[3]{Professor Brunner}
\affil[1]{National Center For Supercomputing Applications (NCSA)}
\affil[2]{Laboratory for Computation, Data, and Machine Learning}
\affil[3]{Illinois Data Science Initiative}
\title{Setting up Spark for Python}

\begin{abstract}
An introduction to setting up Apache Spark with Python
\end{abstract}

\usepackage{listings}
\begin{document}

\begin{titlepage}
\center 
\textsc{\LARGE Illinois Data Science Initiative}\\[1.5cm] 
\textsc{\Large Technical Reports}\\[0.5cm] \HRule \\[0.4cm]
{\huge \bfseries Setting up Spark for Python } \\[0.4cm] \HRule \\[1.5cm]
\Large \emph{Author:}\\ Sameet Sapra \& Joshua Chang \& Professor Brunner\\[3cm]
{\large \today}\\[3cm] % Date
%\includegraphics{Logo}\\[1cm] % uncomment if you want to place a logo
\vfill
\end{titlepage}
%\include{cover}

\maketitle

\section{What is Apache Spark?}

Apache Spark is a fast cluster computing system. Spark can be configured with multiple cluster managers like YARN, Mesos, Amazon EC2, or standalone mode. Along with that it can be configured in local mode and standalone mode. This technical report will install Spark on top of YARN.

Spark supports many high level tools like GraphX and MLlib, for graphs processing and machine learning. It also has many APIs in Java, Scala, and Python, and we will go over PySpark, Python's API for Spark.

\section{Why PySpark?}

PySpark is an API that interfaces with RDD's in Python. It is built on top of Spark's Java API and exposes the Spark programming model to Python. PySpark makes use of a library called Py4J, which enables Python programs to dynamically access Java objects in a Java Virtual Machine. This allows data to be processed in Python and cached in the JVM.

\section{PySpark Prerequisites}

Assume that the environment is a cluster with Hadoop installed. Verify that Java and Scala are installed by checking the versions. If they are installed, skip to the Spark configuration steps, otherwise follow the installation instructions given below.

\section{Installation}

\noindent
Install Java on the system with:
\begin{verbatim}
> sudo yum install java-1.7.0-openjdk-devel
\end{verbatim}

\noindent
Install Spark on the system by downloading the rpm. Here we will install spark-1.6:
\begin{lstlisting}[breaklines]
> wget http://apache.mirrors.ionfish.org/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz
> tar -zxvf spark-1.6.0-bin-hadoop2.6.tgz
> mv spark-1.6.0-bin-hadoop2.6 spark
\end{lstlisting}

\noindent
Let's configure Spark to only show errors on startup, rather than all info. This will make the Spark shell less cluttered when it is opened on startup.
\begin{verbatim}
> cd /usr/hdp/2.3.6.0-3796/spark/conf
> vi log4j.properties
\end{verbatim}

\noindent
Find a line that describe the rootCategory of log:
\begin{verbatim}
log4j.rootCategory=INFO, console
\end{verbatim}

Replace INFO with ERROR:
\begin{verbatim}
log4j.rootCategory=ERROR, console
\end{verbatim}

If all that worked, you should see the Spark shell start up when you type
\begin{verbatim}
> spark-shell
\end{verbatim}

\section{Setting up PySpark}

Now let's ensure that python 2.7 is installed and configured.
\begin{verbatim}
> yum install -y centos-release-SCL
> yum install -y python27
> yum -y install python-pip
\end{verbatim}

\noindent
Finally, let's set up the environment variables for Spark and Python.
\begin{verbatim}
> vi $HOME/.bashrc
> export SPARK_HOME=/usr/hdp/2.3.6.0-3796/spark
> export PYTHONPATH=$SPARK_HOME/python
> export SPARK_HIVE=true
\end{verbatim}

\noindent
Then, reload the environment:
\begin{verbatim}
> source $HOME/.bashrc
\end{verbatim}

Again, let's make sure Python is setup correctly. Type
\begin{verbatim}
> python
\end{verbatim}
You should see the Python REPL come up. Finally, we can start writing code in Python.

\section{Writing our first lines of PySpark}

This example is taken from the Apache Spark website. It runs a parallelized operation to compute the value of $\pi$ and is a good example to see the benefits of Spark.
\begin{lstlisting}[language=python]
def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

count = sc.parallelize(xrange(0, NUM_SAMPLES)) \
             .filter(inside).count()
print "Pi is roughly %f" % (4.0 * count / NUM_SAMPLES)
\end{lstlisting}

Once you've written your Python code, you can compile and deploy it with:
\begin{verbatim}
> spark-submit --master yarn-cluster MY_PYTHON_FILE.py
\end{verbatim}

\noindent
The \textit{'--num-executors 10'} flag (arbitrary number) may be added to specify how many executors, the objects responsible for executing tasks, are to be used. Using as many executors as data nodes is recommended to decrease minimize runtime.

\section{Conclusion}

This technical report serves as a guide to set up an environment to run Spark on HDFS and write some simple Python code to take advantage of Spark using PySpark.

\end{document}
